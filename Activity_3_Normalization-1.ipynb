{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Activity 3: Normalization**\n",
        "**Instructions:**\n",
        "\n",
        "---\n",
        "* Please download the provided IPython Notebook (ipynb) file and open it in Google Colab. Once opened, enter your code in the same file directly beneath the relevant question's code block.\n",
        "\n",
        "\n",
        "* Purpose of this activity is to practice and get hands on experience (Ungraded Activity)  \n",
        "\n",
        "* No Dataset required for this Activity"
      ],
      "metadata": {
        "id": "Ur1Spl3neyQc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8hFNQohPORn"
      },
      "source": [
        "# **Text Preprocessing Beyond Tokenization**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Word** **normalization**\n",
        "Normalization in NLP is the process of converting words into a standard format to improve text analysis. This includes tasks like converting text to lowercase, removing punctuation, stemming, and lemmatization to ensure consistency in language processing.\n",
        "\n",
        "**Word normalization** is a text preprocessing technique used in natural language processing (NLP) to standardize and simplify words or tokens in a text document. The goal of word normalization is to make text data more consistent and manageable for analysis. This process can involve various transformations, such as converting all text to lowercase, removing punctuation, expanding contractions, and performing tasks like stemming or lemmatization to reduce words to their base or dictionary forms. Word normalization helps improve the accuracy and effectiveness of NLP tasks by reducing the complexity of text data and ensuring that similar words are treated as equivalent."
      ],
      "metadata": {
        "id": "HZIYqdWPmHr_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "peiyYN47rMy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fccef03a-809a-4020-f61e-56ecb621538d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# for using NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# for using SpaCy\n",
        "import spacy\n",
        "\n",
        "# for HuggingFace\n",
        "!pip install transformers\n",
        "# !pip install ftfy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It uses NLTK to handle tasks like tokenization, stopword removal, and lemmatization, and downloads specific datasets for these functions (e.g., \"punkt,\" \"stopwords,\" and \"wordnet\"). Additionally, it imports SpaCy for advanced natural language processing and installs the Hugging Face transformers library to work with pre-trained NLP models."
      ],
      "metadata": {
        "id": "Qc6obbYjXnpB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y3Bh5T8sJXFQ"
      },
      "outputs": [],
      "source": [
        "# trick to wrap text to the viewing window for this notebook\n",
        "# Ref: https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results\n",
        "#helps improve the readability and formatting of code output in the notebook.\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code customizes how text and code output are displayed in a Jupyter notebook, making it more readable by enabling line wrapping. It uses the IPython.display module to inject a CSS style that wraps long lines of text (white-space: pre-wrap). The set_css function applies this style before each code cell runs by registering it with the notebook's event system."
      ],
      "metadata": {
        "id": "Of_JLtfpZDhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Revisiting Tokenization** :**TreebankWordTokenizer**\n",
        "\n",
        "The **Treebank Word Tokenizer** is a text processing tool used in natural language processing (NLP) to split text into individual words or tokens. It follows the tokenization conventions and standards of the Penn Treebank corpus"
      ],
      "metadata": {
        "id": "Id3muvWNlNj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "text=\"Hello everyone. Welcome to NLP Course.\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "KiRKo3DKlfuQ",
        "outputId": "055068c0-82b2-466a-c6be-d7b42fc76cff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone.', 'Welcome', 'to', 'NLP', 'Course', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code we uses NLTK's TreebankWordTokenizer to split a given text into individual words or tokens based on specific rules (like punctuation and contractions). It initializes the tokenizer and applies it to the sentence \"Hello everyone. Welcome to NLP Course.\". The output will be a list of tokens (words and punctuation) derived from the text."
      ],
      "metadata": {
        "id": "MMAt-VfIZrrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Using Regular Expression**\n",
        "\n",
        "**RegexpTokenizer** is a text processing tool provided by the Natural Language Toolkit (NLTK) library in Python. It is used to tokenize (split) text into individual tokens (words or phrases) based on a specified regular expression pattern."
      ],
      "metadata": {
        "id": "43OOaeg7o78C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\")#[\\w']+ as a whole matches sequences of word characters (letters, digits, underscores) and single quotes in a string\n",
        "text = \"Let's see how it's working. We also have digits like 123 and 010\"\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "id": "me379siro8Jp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "ea34f506-cef6-41d3-ef7e-480b6a4bf315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Let's\",\n",
              " 'see',\n",
              " 'how',\n",
              " \"it's\",\n",
              " 'working',\n",
              " 'We',\n",
              " 'also',\n",
              " 'have',\n",
              " 'digits',\n",
              " 'like',\n",
              " '123',\n",
              " 'and',\n",
              " '010']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses NLTK's RegexpTokenizer to tokenize text based on a custom regular expression. The pattern [\\w']+ matches sequences of word characters (letters, digits, underscores) and single quotes, ensuring tokens like contractions (Let's) and numbers (123) are captured correctly. Applying this tokenizer to the given text splits it into meaningful tokens while ignoring other characters like punctuation."
      ],
      "metadata": {
        "id": "_kH0tY1tahE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(Tutorial) Tokenizing text using Spacy**\n",
        "\n",
        "Following is a dummy sample of text to demonstrate tokenization in SpaCy."
      ],
      "metadata": {
        "id": "2aGjiBnLylj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_text1 = \"\"\"Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
        "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
        "4th paragraph just has one sentence in it.\n",
        "\"\"\"\n",
        "\n",
        "print(dummy_text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "XeNE7b_xymIL",
        "outputId": "9cf85e9b-5d11-4b8f-f72d-9f9216443b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is the First Paragraph and this is the First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the first paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Now, it is the Second Paragraph and its First Sentence. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the second paragraph. This paragraph is ending now with a Fifth Sentence.\n",
            "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. Here is the Second Sentence. Now is the Third Sentence. This is the Fourth Sentence of the third paragaraph. This paragraph is ending now with a Fifth Sentence.\n",
            "4th paragraph just has one sentence in it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loads a trained English pipeline with specific preprocessing components\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# using SpaCy's tokenizer...\n",
        "doc = nlp(dummy_text1)      # applies the processing pipeline on the text\n",
        "for token in doc:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UwqTRVHdyoCt",
        "outputId": "87847495-14c5-4aa8-9eb7-9946927ee02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here\n",
            "is\n",
            "the\n",
            "First\n",
            "Paragraph\n",
            "and\n",
            "this\n",
            "is\n",
            "the\n",
            "First\n",
            "Sentence\n",
            ".\n",
            "Here\n",
            "is\n",
            "the\n",
            "Second\n",
            "Sentence\n",
            ".\n",
            "Now\n",
            "is\n",
            "the\n",
            "Third\n",
            "Sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "Fourth\n",
            "Sentence\n",
            "of\n",
            "the\n",
            "first\n",
            "paragaraph\n",
            ".\n",
            "This\n",
            "paragraph\n",
            "is\n",
            "ending\n",
            "now\n",
            "with\n",
            "a\n",
            "Fifth\n",
            "Sentence\n",
            ".\n",
            "\n",
            "\n",
            "Now\n",
            ",\n",
            "it\n",
            "is\n",
            "the\n",
            "Second\n",
            "Paragraph\n",
            "and\n",
            "its\n",
            "First\n",
            "Sentence\n",
            ".\n",
            "Here\n",
            "is\n",
            "the\n",
            "Second\n",
            "Sentence\n",
            ".\n",
            "Now\n",
            "is\n",
            "the\n",
            "Third\n",
            "Sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "Fourth\n",
            "Sentence\n",
            "of\n",
            "the\n",
            "second\n",
            "paragraph\n",
            ".\n",
            "This\n",
            "paragraph\n",
            "is\n",
            "ending\n",
            "now\n",
            "with\n",
            "a\n",
            "Fifth\n",
            "Sentence\n",
            ".\n",
            "\n",
            "\n",
            "Finally\n",
            ",\n",
            "this\n",
            "is\n",
            "the\n",
            "Third\n",
            "Paragraph\n",
            "and\n",
            "is\n",
            "the\n",
            "First\n",
            "Sentence\n",
            "of\n",
            "this\n",
            "paragraph\n",
            ".\n",
            "Here\n",
            "is\n",
            "the\n",
            "Second\n",
            "Sentence\n",
            ".\n",
            "Now\n",
            "is\n",
            "the\n",
            "Third\n",
            "Sentence\n",
            ".\n",
            "This\n",
            "is\n",
            "the\n",
            "Fourth\n",
            "Sentence\n",
            "of\n",
            "the\n",
            "third\n",
            "paragaraph\n",
            ".\n",
            "This\n",
            "paragraph\n",
            "is\n",
            "ending\n",
            "now\n",
            "with\n",
            "a\n",
            "Fifth\n",
            "Sentence\n",
            ".\n",
            "\n",
            "\n",
            "4th\n",
            "paragraph\n",
            "just\n",
            "has\n",
            "one\n",
            "sentence\n",
            "in\n",
            "it\n",
            ".\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code loads SpaCy's pre-trained English language model (en_core_web_sm), which includes tools for tasks like tokenization, part-of-speech tagging, and named entity recognition. It processes the text stored in dummy_text1 using this model, creating a doc object that represents the entire processed text. Finally, it iterates through the doc to print each token (word or punctuation) individually."
      ],
      "metadata": {
        "id": "IYhQYi2_bVKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** Copy the above snippet code and Modify the above regular expression to match only **digits** from the above given text.  "
      ],
      "metadata": {
        "id": "jaHfuJo-phTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Let's see how it's working. We also have digits like 123 and 010\"\n",
        "\n",
        "# Regular expression to match only digits\n",
        "tokenizer = RegexpTokenizer(r'\\d+')\n",
        "\n",
        "# Tokenizing the text to extract only digits\n",
        "digits = tokenizer.tokenize(text)\n",
        "print(digits)"
      ],
      "metadata": {
        "id": "ZiQuSrzQphoo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8bf75ac-a514-455d-e9ff-addec07d8401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['123', '010']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses NLTK's RegexpTokenizer with a regular expression (\\d+) to extract only digits from a given text. The pattern \\d+ matches one or more consecutive digits in the string. Applying this tokenizer to the text \"Let's see how it's working. We also have digits like 123 and 010\" results in a list containing only the numbers (123 and 010)."
      ],
      "metadata": {
        "id": "ZNyla9yTc9te"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgaX-Ck7YYzY"
      },
      "source": [
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNUmlAHIQ9gs"
      },
      "source": [
        "## ** (Tutorial) Stemming and Lemmatization using NLTK**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming** is a text normalization technique in natural language processing and information retrieval. It involves reducing words to their root or base form, often by removing suffixes or prefixes. The goal of stemming is to convert words with the same meaning but different forms into a common base form so that they can be treated as equivalent during text analysis and retrieval. Stemming helps improve information retrieval and text processing tasks by reducing the complexity of words while maintaining their core meaning. Common stemming algorithms include the Porter Stemmer and Snowball Stemmer."
      ],
      "metadata": {
        "id": "Rxa-khLuaEZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Porter Stemmer**\n",
        "The **Porter Stemmer** is a well-known algorithm for stemming in natural language processing. It was  designed to reduce words to their root or base form by removing common suffixes. Stemming is the process of reducing words to their linguistic root or base form to simplify text analysis and improve information retrieval.For example, it can convert words like \"running,\" \"runs,\" and \"ran\" to their common root \"run.\""
      ],
      "metadata": {
        "id": "7TPOS73taHEq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jCebsYwiQxf"
      },
      "source": [
        "Let's see how we can perform stemming and lemmatization using NLTK library..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3M0mDUIiSdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7aa67d26-e65a-473f-b526-a9c847b1c03a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'cats' after stemming: cat\n",
            "'better' after stemming: better\n",
            "'abaci' after stemming: abaci\n",
            "'aardwolves' after stemming: aardwolv\n",
            "'generically' after stemming: gener\n"
          ]
        }
      ],
      "source": [
        "# importing PorterStemmer class from nltk.stem module\n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()    # instantiating an object of the PorterStemmer class\n",
        "\n",
        "stem = porter.stem('cats')    # calling the stemmer algorithm on the desired word\n",
        "print(f\"'cats' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('better')\n",
        "print(f\"'better' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('abaci')\n",
        "print(f\"'abaci' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('aardwolves')\n",
        "print(f\"'aardwolves' after stemming: {stem}\")\n",
        "\n",
        "stem = porter.stem('generically')\n",
        "print(f\"'generically' after stemming: {stem}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses NLTK's PorterStemmer to perform stemming, which reduces words to their root form. It creates an instance of the PorterStemmer class and applies its stem method to several words (cats, better, abaci, aardwolves, generically). The code prints the stemmed version of each word, showing how the stemming algorithm simplifies words by removing suffixes or applying linguistic rules."
      ],
      "metadata": {
        "id": "OFmuy6OGdjIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Lemmatization**\n",
        "**Lemmatization** is a natural language processing technique that reduces words to their base or dictionary form, known as a \"lemma.\" Unlike stemming, which often involves removing suffixes to approximate a word's root, lemmatization considers the word's context and grammatical meaning. The goal is to transform different inflected forms of a word into a common base form. Lemmatization is particularly useful for maintaining the grammatical correctness of words in text analysis and information retrieval tasks."
      ],
      "metadata": {
        "id": "WOhnG5IIcaHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**WordNet Lemmatizer**\n",
        "The **WordNet Lemmatizer** is a lemmatization tool based on WordNet, a lexical database of the English language. WordNet groups words into sets of synonyms called \"synsets\" and provides a rich lexical and semantic structure for the English language. The WordNet Lemmatizer uses this semantic information to perform lemmatization, which is the process of reducing words to their base or dictionary form (lemma)"
      ],
      "metadata": {
        "id": "tURX72sldJvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "8wVrYUPXS2Oy",
        "outputId": "8873ca52-203d-4688-beca-6f0ad938e601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Jo-qgQfjuli",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "6b3c0c55-5606-4b6f-8e9d-d87e03152847"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'cats' after lemmatization: cat\n",
            "'better' after lemmatization: better\n",
            "'abaci' after lemmatization: abacus\n",
            "'aardwolves' after lemmatization: aardwolf\n",
            "'generically' after lemmatization: generically\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "'better' (as an adjective) after lemmatization: good\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# importing Word Net-based lemmatizer class from nltk.stem module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()    # instantiating an object of the Word NetLemmatizer class\n",
        "\n",
        "lemma = lemmatizer.lemmatize('cats')    # calling the lemmatization algorithm on the desired word\n",
        "print(f\"'cats' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('better')\n",
        "print(f\"'better' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('abaci')\n",
        "print(f\"'abaci' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('aardwolves')\n",
        "print(f\"'aardwolves' after lemmatization: {lemma}\")\n",
        "\n",
        "lemma = lemmatizer.lemmatize('generically')\n",
        "print(f\"'generically' after lemmatization: {lemma}\")\n",
        "\n",
        "print(\"\\n\\n\\n\")\n",
        "lemma = lemmatizer.lemmatize('better', pos='a')   # 'a' denoted ADJECTIVE part-of-speech\n",
        "print(f\"'better' (as an adjective) after lemmatization: {lemma}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code demonstrates lemmatization using NLTK's WordNetLemmatizer, which reduces words to their base or dictionary form (lemma) based on context and meaning. It creates an instance of WordNetLemmatizer and applies the lemmatize method to several words (cats, better, abaci, aardwolves, generically), printing their lemmatized forms.\n",
        "\n",
        "The code also shows how specifying the part of speech (POS) for a word can improve lemmatization accuracy. For example, when lemmatizing the word better and specifying it as an adjective (pos='a'), the lemmatizer correctly identifies its base form as good. Without the POS information, it assumes the default POS is a noun, which may not always give the correct result."
      ],
      "metadata": {
        "id": "-gzhJDIHgj3E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDGVeoHqrQkF"
      },
      "source": [
        "### **Stemming on text string**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def stem_text(text):\n",
        "    # Initialize the Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Apply the stemmer to each word and join them back into a text\n",
        "    stemmed_text = ' '.join([stemmer.stem(word) for word in words])\n",
        "\n",
        "    return stemmed_text\n",
        "\n",
        "# Example usage:\n",
        "text = \"He is jumping, and he jumped over the jumps.\"\n",
        "stemmed_text = stem_text(text)\n",
        "print(stemmed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "0OILkAMtu6iS",
        "outputId": "01bb0507-2784-4759-dcd0-325fddfd916b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he is jump , and he jump over the jump .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines a function stem_text that processes a sentence by first breaking it into individual words using word_tokenize. It then applies the PorterStemmer to each word, reducing them to their root forms (e.g., \"jumping\" becomes \"jump\"). Finally, the stemmed words are joined back into a sentence, and the stemmed version of the input text is printed."
      ],
      "metadata": {
        "id": "l0pDXkhukYYL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCtK0QouYj2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fad3c944-1666-49b0-cbf6-048ba74b6509"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given text:\n",
            "In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form; generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\n"
          ]
        }
      ],
      "source": [
        "# This is the text on which you have to perform stemming; taken from Wikipedia.\n",
        "text = \"In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form; generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\"\n",
        "print(\"Given text:\")\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tutorial **"
      ],
      "metadata": {
        "id": "Cl9AIuruwUrE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el7w7c7HmY9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6351ed9a-7668-47af-d177-45ef50ca6d48"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After punctuation removal:\n",
            "in linguistic morphology and information retrieval stemming is the process of reducing inflected or sometimes derived words to their word stem base or root form generally a written word form the stem need not be identical to the morphological root of the word it is usually sufficient that related words map to the same stem even if this stem is not in itself a valid root\n",
            "\n",
            "\n",
            "After stopword removal:\n",
            "['linguistic', 'morphology', 'information', 'retrieval', 'stemming', 'process', 'reducing', 'inflected', 'sometimes', 'derived', 'words', 'word', 'stem', 'base', 'root', 'form', 'generally', 'written', 'word', 'form', 'stem', 'need', 'identical', 'morphological', 'root', 'word', 'usually', 'sufficient', 'related', 'words', 'map', 'stem', 'even', 'stem', 'valid', 'root']\n"
          ]
        }
      ],
      "source": [
        "#CODE BLOCK 1\n",
        "en_stopwords = set(stopwords.words('english'))\n",
        "def remove_punc(text_string):\n",
        "  return re.sub('[^a-zA-Z0-9 ]', '', text_string.lower())\n",
        "\n",
        "def remove_stopwords(text_string):\n",
        "  return [ token for token in text_string.split(' ') if token not in en_stopwords ]\n",
        "\n",
        "# applying punctuation removal to the text\n",
        "unpunc_text = remove_punc(text)\n",
        "print(\"After punctuation removal:\")\n",
        "print(unpunc_text)\n",
        "\n",
        "# # applying stopword removal to the text\n",
        "clean_text = remove_stopwords(unpunc_text)\n",
        "print(\"\\n\\nAfter stopword removal:\")\n",
        "print(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines two functions: one to remove punctuation and another to remove stopwords. The remove_punc function takes a text string, converts it to lowercase, and removes any character that isn't a letter, number, or space. The remove_stopwords function splits the text into individual words and filters out common stopwords (e.g., \"the\", \"is\", \"and\"). The code then applies these functions to the input text, printing the text after punctuation removal and after stopwords are removed."
      ],
      "metadata": {
        "id": "Az3mn2eXkocz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac5uju7eGVfg"
      },
      "source": [
        "#### **Question 2. Perform stemming on the cleaned text(from tutorial2-code block 1) above using the Porter Stemmer from NLTK.**\n",
        "\n",
        "**Hint:** import PorterStemmer from nltk.stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAC8FFLCErdI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "df8ab207-0e4d-4299-b6ac-3d1593817384"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "After stemming:\n",
            "['linguist', 'morpholog', 'inform', 'retriev', 'stem', 'process', 'reduc', 'inflect', 'sometim', 'deriv', 'word', 'word', 'stem', 'base', 'root', 'form', 'gener', 'written', 'word', 'form', 'stem', 'need', 'ident', 'morpholog', 'root', 'word', 'usual', 'suffici', 'relat', 'word', 'map', 'stem', 'even', 'stem', 'valid', 'root']\n"
          ]
        }
      ],
      "source": [
        "# apply Porter Stemmer on the cleaned text (after punctuation and stopwords are removed) below this comment\n",
        "#CODE HERE\n",
        "# Stemming using PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_text = [stemmer.stem(word) for word in clean_text]\n",
        "\n",
        "# Print the stemmed text\n",
        "print(\"\\n\\nAfter stemming:\")\n",
        "print(stemmed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code applies the Porter Stemmer to the cleaned text (which has had punctuation and stopwords removed). It loops through each word in the cleaned text, reducing it to its root form using the stemmer. The result is a list of stemmed words, which is then printed."
      ],
      "metadata": {
        "id": "VULgCQSRsYhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization on text string**"
      ],
      "metadata": {
        "id": "4bUoRnNN6t0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "ib7DcVk3r9Aj",
        "outputId": "72dc677a-85ef-4e22-83c4-da24aca9588c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example text to lemmatize\n",
        "text = \"There are like more than 100 foxes and lions in this forest.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = text.split()\n",
        "\n",
        "# Lemmatize each word and join them back into a sentence\n",
        "lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "# Print the original and lemmatized text\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Lemmatized Text:\", lemmatized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "fp2nzJba6sJZ",
        "outputId": "52767516-e8f4-4ac8-8b81-94686f6b17a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: There are like more than 100 foxes and lions in this forest.\n",
            "Lemmatized Text: There are like more than 100 fox and lion in this forest.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code takes a sentence and breaks it down into individual words using the split() method. It then lemmatizes each word using the WordNetLemmatizer to reduce them to their base forms (e.g., \"foxes\" becomes \"fox\"). Finally, the lemmatized words are combined back into a sentence and both the original and lemmatized texts are printed."
      ],
      "metadata": {
        "id": "lAG5oC7jj7r3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N44TNDasS6eK"
      },
      "source": [
        "#### **Question 3. Perform lemmatization on the same cleaned text(from tutorial2-code block 1) above using NLTK's lemmatizer.**\n",
        "\n",
        "**Hint**:import WordNetLemmatizer from nltk.stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ-moA25Erh3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bbee14ce-b557-48f8-a957-c7414be28d66"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "After lemmatization:\n",
            "['linguistic', 'morphology', 'information', 'retrieval', 'stemming', 'process', 'reducing', 'inflected', 'sometimes', 'derived', 'word', 'word', 'stem', 'base', 'root', 'form', 'generally', 'written', 'word', 'form', 'stem', 'need', 'identical', 'morphological', 'root', 'word', 'usually', 'sufficient', 'related', 'word', 'map', 'stem', 'even', 'stem', 'valid', 'root']\n"
          ]
        }
      ],
      "source": [
        "# apply NLTK's lemmatizer on the cleaned text (after punctuation and stopwords are removed) below this comment\n",
        "#CODE HERE\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize each word in the cleaned text\n",
        "lemmatized_text = [lemmatizer.lemmatize(word) for word in clean_text]\n",
        "\n",
        "# Print the lemmatized text\n",
        "print(\"\\n\\nAfter lemmatization:\")\n",
        "print(lemmatized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses NLTK's WordNetLemmatizer to reduce each word in the cleaned text to its base form, called a lemma. It loops through each word in the cleaned text, applying the lemmatizer to get the root form of the word (e.g., \"running\" becomes \"run\"). Finally, it prints the lemmatized version of the text."
      ],
      "metadata": {
        "id": "Nr_Kld7vszWI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJNe9XfmOcqi"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYE8A-Mk4BKe"
      },
      "source": [
        "## **(Tutorial) Subword Tokenization using HuggingFace**\n",
        "\n",
        "**Hugging Face** is used for subword tokenization by offering NLP practitioners access to pre-trained subword tokenizers and models. Hugging Face's \"transformers\" library offers pre-trained models and tokenizers, such as Byte Pair Encoding (BPE) and SentencePiece, which are widely used for subword tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subword tokenization** is a text processing technique used in natural language processing (NLP) to break down words into smaller units, often subword pieces. This approach is particularly useful for handling languages with complex morphology or when dealing with out-of-vocabulary words. Subword tokenization methods like Byte-Pair Encoding (BPE) and SentencePiece divide text into subword units, such as character-level tokens or subword pieces, allowing NLP models to work with a more extensive and adaptable vocabulary. This technique improves the handling of rare words and enhances the performance of NLP models on a wide range of languages and tasks"
      ],
      "metadata": {
        "id": "Y48GHrsfgHI5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV2KUGEhIm2M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "424606c0-2440-481d-e7e8-7dbd659f0ab1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.12.14)\n",
            "--2025-01-28 16:22:07--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.202.248, 3.5.16.7, 16.182.73.136, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.202.248|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‘gpt2-medium-vocab.json’\n",
            "\n",
            "gpt2-medium-vocab.j 100%[===================>]   1018K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-01-28 16:22:07 (8.64 MB/s) - ‘gpt2-medium-vocab.json’ saved [1042301/1042301]\n",
            "\n",
            "--2025-01-28 16:22:07--  https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.202.248, 3.5.16.7, 16.182.73.136, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.202.248|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [text/plain]\n",
            "Saving to: ‘gpt2-merges.txt’\n",
            "\n",
            "gpt2-merges.txt     100%[===================>] 445.62K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-01-28 16:22:07 (5.58 MB/s) - ‘gpt2-merges.txt’ saved [456318/456318]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers\n",
        "#This is a JSON file that contains the vocabulary (i.e., the set of words and subword pieces) used by the GPT-2 model\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code installs the tokenizers library, which is used for tokenizing text data. It then downloads two files: gpt2-medium-vocab.json, which contains the vocabulary (set of words and subword pieces) for the GPT-2 model, and gpt2-merges.txt, which contains the merge rules for combining subword pieces into words. These files are necessary for working with the GPT-2 model's tokenizer."
      ],
      "metadata": {
        "id": "xIdew6AJt0T3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Byte Pair Encoding (BPE)** is a subword tokenization technique used in natural language processing (NLP) and text processing. It involves dividing text into subword units, typically based on frequency, to create a more flexible and adaptive vocabulary for language models."
      ],
      "metadata": {
        "id": "0wdDDf6VgnwQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyiWXf-hLtRF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "025aae13-6241-4aae-d7c2-be45fdcb9b78"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[464, 2183, 286, 13630, 281, 2209, 319, 554, 7493, 3924, 3596, 2067, 351, 262, 845, 717, 554, 7493, 3924, 960, 20191, 2669, 447, 247, 82, 960, 261, 3035, 1542, 11, 1596, 4531, 13]\n",
            "['The', 'Ġcustom', 'Ġof', 'Ġdelivering', 'Ġan', 'Ġaddress', 'Ġon', 'ĠIn', 'aug', 'uration', 'ĠDay', 'Ġstarted', 'Ġwith', 'Ġthe', 'Ġvery', 'Ġfirst', 'ĠIn', 'aug', 'uration', 'âĢĶ', 'George', 'ĠWashington', 'âĢ', 'Ļ', 's', 'âĢĶ', 'on', 'ĠApril', 'Ġ30', ',', 'Ġ17', '89', '.']\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "gpt2vocab = \"gpt2-medium-vocab.json\"\n",
        "gpt2merges = \"gpt2-merges.txt\"\n",
        "\n",
        "bpe = ByteLevelBPETokenizer(gpt2vocab, gpt2merges)\n",
        "bpe_encoding = bpe.encode(\"The custom of delivering an address on Inauguration Day started with the very first Inauguration—George Washington’s—on April 30, 1789.\")\n",
        "print(bpe_encoding.ids)\n",
        "print(bpe_encoding.tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses the ByteLevelBPETokenizer from the tokenizers library to initialize a tokenizer using the vocabulary (gpt2-medium-vocab.json) and merge rules (gpt2-merges.txt) specific to the GPT-2 model. It then encodes a sample sentence, breaking it into tokens and assigning each token a unique ID. Finally, the code prints the token IDs (bpe_encoding.ids) and the actual tokens (bpe_encoding.tokens) for the given sentence."
      ],
      "metadata": {
        "id": "JLO4iufWuCHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**: Collect the encoding ids which you generate from above snippet and now decode the ids to get back the given text string?\n",
        "\n",
        "**Hint**: use decode() method"
      ],
      "metadata": {
        "id": "lvqxvTMqj5qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE HERE\n",
        "# Decoding the encoding ids back to the original text\n",
        "decoded_text = bpe.decode(bpe_encoding.ids)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "id": "GaRsld66j5Ih",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "895edf89-6a3f-4c19-fb9a-9afd87fe3dba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The custom of delivering an address on Inauguration Day started with the very first Inauguration—George Washington’s—on April 30, 1789.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses the decode() method of the ByteLevelBPETokenizer to convert the list of token IDs (bpe_encoding.ids) back into the original text. It takes the encoding IDs that were generated from the sentence and decodes them into their corresponding tokens to reconstruct the original string. The decoded text is then printed, which should match the original sentence."
      ],
      "metadata": {
        "id": "QWvDylC8ucVi"
      }
    }
  ]
}